{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from bisect import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb_Actions: 4\n",
      "Nb_States: 16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    About FrozenLake: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "    SFFF       (S: starting point, safe)\n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole, fall to your doom)\n",
    "    HFFG       (G: goal, where the frisbee is located)\n",
    "    \n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "n_a = env.action_space.n\n",
    "n_s = env.observation_space.n\n",
    "print(\"Nb_Actions: {}\".format(n_a))\n",
    "print(\"Nb_States: {}\".format(n_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_action(state, epsilon, Q):\n",
    "    return env.action_space.sample() if np.random.uniform(0,1) < epsilon else np.argmax(Q[state,:])\n",
    "\n",
    "def normal_action(state, i, Q):\n",
    "    return np.argmax(Q[state, :] + np.random.randn(1, n_a) * (1. / (i + 1)))\n",
    "\n",
    "\n",
    "def softmax(Q,s,a):\n",
    "    return np.exp(1/tau*Q[s,a])\n",
    "\n",
    "def softmax_action(s,Q):\n",
    "    all_actions = list(range(n_a))\n",
    "    den = sum([softmax(Q,s,b) for b in all_actions])\n",
    "    for a in all_actions:\n",
    "        P[s,a] = softmax(Q,s,a)/den\n",
    "    cum_probas = np.cumsum(P[s,:])\n",
    "    return bisect(cum_probas, np.random.uniform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(Q, state, action, new_state, new_action, reward):\n",
    "    return Q[state, action] + alpha * (reward + gamma * Q[new_state, new_action] - Q[state, action])\n",
    "\n",
    "def q_learning(Q, state, action, new_state, new_action, reward):\n",
    "    return Q[state, action] + alpha * (reward + gamma * max(Q[new_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5000 | Success rate of the last 5000 episodes: 0.0168\n",
      " 10000 | Success rate of the last 5000 episodes: 0.0166\n",
      " 15000 | Success rate of the last 5000 episodes: 0.0174\n",
      " 20000 | Success rate of the last 5000 episodes: 0.0172\n",
      " 25000 | Success rate of the last 5000 episodes: 0.02\n",
      " 30000 | Success rate of the last 5000 episodes: 0.0186\n",
      " 35000 | Success rate of the last 5000 episodes: 0.0212\n",
      " 40000 | Success rate of the last 5000 episodes: 0.0184\n",
      " 45000 | Success rate of the last 5000 episodes: 0.0196\n",
      " 50000 | Success rate of the last 5000 episodes: 0.0192\n",
      " 55000 | Success rate of the last 5000 episodes: 0.017\n",
      " 60000 | Success rate of the last 5000 episodes: 0.0166\n",
      " 65000 | Success rate of the last 5000 episodes: 0.0196\n",
      " 70000 | Success rate of the last 5000 episodes: 0.0224\n",
      " 75000 | Success rate of the last 5000 episodes: 0.02\n",
      " 80000 | Success rate of the last 5000 episodes: 0.0166\n",
      " 85000 | Success rate of the last 5000 episodes: 0.0214\n",
      " 90000 | Success rate of the last 5000 episodes: 0.0152\n",
      " 95000 | Success rate of the last 5000 episodes: 0.0178\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9            # e-greedy stratgy\n",
    "tau     = 0.9            # softmax strategy\n",
    "P = np.zeros((n_s,n_a))  # softmax strategy\n",
    "\n",
    "# Define a Q-table to host the Q-function estimate\n",
    "Q = np.zeros((n_s,n_a))\n",
    "\n",
    "gamma   = 0.99 # discount factor\n",
    "alpha   = 0.85 # learning reate\n",
    "\n",
    "nb_episodes = 100000\n",
    "\n",
    "# Store the full rewards of the episodes\n",
    "all_rewards = []\n",
    "print_freq  = 5000\n",
    "\n",
    "for i in range(nb_episodes):\n",
    "    state       = env.reset() # In this case, observation = state\n",
    "    full_reward = 0\n",
    "    timestep    = 0\n",
    "    done        = False\n",
    "    #action      = normal_action(state, i, Q)\n",
    "    #action      = epsilon_action(state, epsilon, Q)\n",
    "    action      = softmax_action(state, Q)\n",
    "\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        \"\"\" Selects an action \"\"\" \n",
    "        #new_action = normal_action(new_state, i, Q)\n",
    "        #new_action = epsilon_action(new_state, Q)\n",
    "        new_action = softmax_action(new_state, Q)\n",
    "        \n",
    "        \"\"\" Update the tabular estimate of the Q-function \"\"\"\n",
    "        #Q[state, action] = sarsa(Q, state, action, new_state, new_action, reward)\n",
    "        Q[state, action] = q_learning(Q, state, action, new_state, new_action, reward)\n",
    "        \n",
    "\n",
    "        full_reward += reward\n",
    "        timestep    += 1\n",
    "        state       = new_state\n",
    "        action      = new_action\n",
    "\n",
    "    all_rewards.append(full_reward)\n",
    "    #print(\"Episode {} finished after {} timesteps\".format(i, timestep))\n",
    "    if i % print_freq == 0 and i is not 0:\n",
    "        print(\"{:6d} | Success rate of the last {} episodes: {}\".format(i, print_freq, np.mean(all_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
