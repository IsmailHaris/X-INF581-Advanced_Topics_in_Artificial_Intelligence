{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from copy import deepcopy\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "## Set up an Environment to work on\n",
    "######################################################################################################\n",
    "\n",
    "class Environment:\n",
    "    \n",
    "    # MDP = Markov Chain + One-step Decision Theory\n",
    "\n",
    "    nS = 3\n",
    "    nA = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Transition function: P(s'|s, a)\n",
    "        self.P = random.rand(self.nS,self.nA,self.nS)\n",
    "        self.P = self.P * (self.P > 0.9)\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                self.P[s,a,:] = random.rand(self.nS) \n",
    "                self.P[s,a,:] = self.P[s,a,:] / sum(self.P[s,a,:])\n",
    "        \n",
    "        # reward function: R(s, a, s') \n",
    "        self.R = zeros((self.nS,self.nA,self.nS)) \n",
    "        self.R[:,0,self.nS-1] = 1\n",
    "        self.R[:,1,self.nS-1] = 1\n",
    "\n",
    "    def draw_graph(self, fname):\n",
    "        ''' Draw the graph.\n",
    "        '''\n",
    "        with open(fname, \"w\") as text_file:\n",
    "            text_file.write(\"digraph MDP {\\n\")\n",
    "            for s in range(self.nS):\n",
    "                text_file.write(\"\\ts_%s [style=filled shape=circle fillcolor=lightblue] ;\\n\" % (s+1)) \n",
    "                for a in range(self.nA):\n",
    "                    text_file.write(\"\\ta_%s%s [label=\\\"a_%d\\\", style=filled, shape=diamond, fillcolor=indianred1, fontsize=10, fixedsize=true, width=0.5, height=0.5] ;\\n\" % (s+1,a+1,a+1)) \n",
    "                    text_file.write(\"\\ts_%s -> a_%s%s ;\\n\" % (s+1,a+1,a+1)) \n",
    "                    for s_ in range(self.nS):\n",
    "                        if self.R[s,a,s_] > 0:\n",
    "                            text_file.write(\"\\ta_%s%s -> s_%s [label=\\\"%2.1f (r=%2.1f)\\\" color=green] ;\\n\" % (s+1,a+1,s_+1,self.P[s,a,s_],self.R[s,a,s_])) \n",
    "                        else:\n",
    "                            text_file.write(\"\\ta_%s%s -> s_%s [label=\\\"%2.1f\\\"] ;\\n\" % (s+1,a+1,s_+1,self.P[s,a,s_])) \n",
    "            text_file.write(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "env.draw_graph(\"markov_decision_process.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([6.67835378, 6.77772066, 6.63588305]), array([0, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "## Task 1 : Value Iteration\n",
    "######################################################################################################\n",
    "\n",
    "# Objective: obtain a policy pi*: S -> A, indicating which action to take in a given state\n",
    "# Apply Bellman optimality equation Eq. iteratively, until convergence.\n",
    "\n",
    "epsilon = 0.05\n",
    "gamma   = 0.9\n",
    "\n",
    "def value_iteration():\n",
    "\n",
    "    V = zeros(env.nS)\n",
    "    pi = zeros(env.nS, dtype='int64')\n",
    "    W = zeros(env.nS)\n",
    "    values = zeros(env.nA)\n",
    "\n",
    "    while \"No convergence\":\n",
    "\n",
    "        V = deepcopy(W)\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            for a in range(env.nA):\n",
    "                values[a] = sum([env.P[s,a,t]*(env.R[s,a,t] + gamma*V[t]) for t in range(env.nS)])\n",
    "\n",
    "            pi[s] = argmax(values)\n",
    "            W[s] = values[pi[s]]\n",
    "\n",
    "        if linalg.norm(V-W) < epsilon:\n",
    "            break\n",
    "\n",
    "    return W,pi\n",
    "\n",
    "print(value_iteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([6.91564801, 7.0150149 , 6.87317729]), array([0, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "## Task 2 : Policty iteration\n",
    "######################################################################################################\n",
    "\n",
    "def policy_iteration():\n",
    "\n",
    "    V = zeros(env.nS)\n",
    "    pi = zeros(env.nS, dtype='int64')\n",
    "    W = zeros(env.nS)\n",
    "    i = 0\n",
    "    \n",
    "    updated = True\n",
    "    \n",
    "    while updated:\n",
    "        updated = False\n",
    "        i += 1\n",
    "        V = deepcopy(W)\n",
    "        \n",
    "        # Policty evaluation\n",
    "        for s in range(env.nS):\n",
    "            W[s] = sum([env.P[s,pi[s],t]*(env.R[s,pi[s],t] + gamma*V[t]) for t in range(env.nS)])\n",
    "        \n",
    "        # Greedy Pi update\n",
    "        for s in range(env.nS):\n",
    "            q_best = W[s]\n",
    "\n",
    "            for a in range(env.nA):\n",
    "                q_sa = sum([env.P[s,a,t]*(env.R[s,a,t] + gamma*W[t]) for t in range(env.nS)])\n",
    "\n",
    "                if q_sa > q_best:\n",
    "                    pi[s] = a\n",
    "                    q_best = q_sa\n",
    "                    updated = True\n",
    "\n",
    "    return W,pi\n",
    "\n",
    "print(policy_iteration())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
