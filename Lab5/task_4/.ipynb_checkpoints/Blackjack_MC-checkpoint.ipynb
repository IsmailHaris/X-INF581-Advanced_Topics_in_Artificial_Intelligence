{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "#from visualizeValue import visualizeValue\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "epochs = 100000 # Number of episodes/plays\n",
    "epsilon = 1. # E-greedy\n",
    "gamma = 1.0 # Discount factor \n",
    "\n",
    "############# SOLUTION AS FOLLOWS ####################################################\n",
    "alpha = 0.1\n",
    "\n",
    "# Initialize the Q table\n",
    "# N.B. The state is in fact 21 * 10 * 2 scenarios (Current score * Card value * Usable Ace or Not), there are 2 actions\n",
    "Q = np.zeros((21,10,2,2)) \n",
    "\n",
    "# Initialize the policy\n",
    "def policy(s):\n",
    "    '''\n",
    "        Make soft policy $\\pi(s,a)$\n",
    "    '''\n",
    "    p_a = np.ones(2, dtype=float) * epsilon / 2\n",
    "    a_max = np.argmax(Q[s[0]-1, s[1]-1, int(s[2]),:])\n",
    "    p_a[a_max] += (1.0 - epsilon)\n",
    "    return p_a\n",
    "\n",
    "# Initialize the sets\n",
    "R_sum = np.zeros((21,10,2,2))\n",
    "R_cnt = np.zeros((21,10,2,2))\n",
    "\n",
    "# For each episode (i.e., each game)\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Initialize new game/episode (choose and observe a random initial state)\n",
    "    episode = []        \n",
    "    s  = env.reset() \n",
    "\n",
    "    # Generate the episode (using e-soft policy)\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Act with the epsilon-soft policy\n",
    "        p_a = policy(s)\n",
    "        a = np.random.choice(np.arange(2), p=p_a)\n",
    "\n",
    "        # Take the action, get the reward and next state\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        # ... and save the (s, a, r) tuple\n",
    "        episode.append((s, a, r))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Set the next state\n",
    "        s = s_\n",
    "\n",
    "    # For each (s,a) pair in the episode\n",
    "    sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "    for s,a in sa_in_episode:\n",
    "        sa_pair = (s, a)\n",
    "        # Find the first occurence         \n",
    "        first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s and x[1] == a)\n",
    "        # Calculate the return \n",
    "        R = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "        # Store in the set\n",
    "        R_sum[sa_pair] += R\n",
    "        R_cnt[sa_pair] += 1\n",
    "        # Policy improvement\n",
    "        #Q[s][a] = R_sum[sa_pair] / R_cnt[sa_pair]\n",
    "        Q[s[0]-1, s[1]-1, int(s[2]),a] = R_sum[sa_pair] / R_cnt[sa_pair]\n",
    "\n",
    "    # Decay epsilon\n",
    "    #epsilon = epsilon*0.9999\n",
    "   \n",
    "    if i > 0 and (i % 1000) == 0:\n",
    "        print(\"Episode: \", i, \"Average Return: \", R_sum/i)\n",
    "    \n",
    "#printQ(Q) \n",
    "\n",
    "#visualizeValue(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
